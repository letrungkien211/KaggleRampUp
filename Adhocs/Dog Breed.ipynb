{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout, Input, AveragePooling2D, Lambda\n",
    "from urllib import request\n",
    "from keras.callbacks import TensorBoard\n",
    "import tempfile\n",
    "from keras import optimizers\n",
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from keras.applications import vgg16, resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on kaggle kernel: True\n"
     ]
    }
   ],
   "source": [
    "NOT_KAGGLE_KERNEL = os.environ.get('NOT_KAGGLE_KERNEL', 'false') == 'true'\n",
    "print('Run on kaggle kernel:', NOT_KAGGLE_KERNEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../input/dog-breed' if NOT_KAGGLE_KERNEL else '../input'\n",
    "EPOCHS = 100\n",
    "IMAGE_SIZE = (224,224)\n",
    "INPUT_SHAPE = IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "    if not os.path.exists(ROOT_DIR):\n",
    "        os.makedirs(ROOT_DIR)\n",
    "        zip_path = os.path.join(tempfile.gettempdir(), 'dog-breed.zip')\n",
    "        print('Start download!')\n",
    "        request.urlretrieve('https://kienle.blob.core.windows.net/public/kaggle/dog-breed.zip', zip_path)\n",
    "        print('Start unzip')\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(ROOT_DIR)\n",
    "        with zipfile.ZipFile(os.path.join(ROOT_DIR,'test.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall(ROOT_DIR)\n",
    "        with zipfile.ZipFile(os.path.join(ROOT_DIR,'train.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall(ROOT_DIR)\n",
    "        os.remove(zip_path)\n",
    "        os.remove(os.path.join(ROOT_DIR,'train.zip'))\n",
    "        os.remove(os.path.join(ROOT_DIR,'test.zip'))\n",
    "        print('Done')\n",
    "if(NOT_KAGGLE_KERNEL):\n",
    "    fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Confirm all data are ready \n",
    "# Expected output: labels.csv  sample_submission.csv  test  train\n",
    "!ls $ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_images = pd.read_csv(os.path.join(ROOT_DIR, 'labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10222 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "train_valid_generator = ImageDataGenerator().flow_from_dataframe(pd_images, os.path.join(ROOT_DIR, 'train'), batch_size=32, x_col='id', y_col='breed', has_ext=False, target_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_valid_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtractor():\n",
    "    def __init__(self, input_shape, pretrained_model, preprocess_input):\n",
    "        self.input_shape = input_shape\n",
    "        model = pretrained_model(include_top=False, input_shape=self.input_shape, weights='imagenet')\n",
    "        inputs = Input(self.input_shape)\n",
    "        x = inputs\n",
    "        x = Lambda(preprocess_input, name='preprocessing')(x)\n",
    "        x = model(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        model = Model(inputs, x)\n",
    "        self.model = model\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trule\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "vgg_features_extractor = FeaturesExtractor(INPUT_SHAPE, vgg16.VGG16, vgg16.preprocess_input)\n",
    "resnet50_features_extractor = FeaturesExtractor(INPUT_SHAPE, resnet50.ResNet50, resnet50.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model(num_classes, input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    x = inputs\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 320/320 [00:43<00:00,  7.42it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.zeros((train_valid_generator.n,) + INPUT_SHAPE)\n",
    "y_train = np.zeros((train_valid_generator.n, 120))\n",
    "for i in tqdm(range(len(train_valid_generator)), ncols=100):\n",
    "    temp_x, temp_y = train_valid_generator[i]\n",
    "    start_index = i*train_valid_generator.batch_size\n",
    "    X_train[start_index:start_index + temp_x.shape[0]] = temp_x\n",
    "    y_train[start_index:start_index + temp_x.shape[0]] = temp_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate vgg features\n",
    "vgg_features = vgg_features_extractor.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 61,560\n",
      "Trainable params: 61,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_based_model = create_simple_model(num_classes, vgg_features_extractor.model.output.get_shape().as_list()[1:])\n",
    "vgg_based_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "if NOT_KAGGLE_KERNEL:\n",
    "    callbacks = [TensorBoard('./data/logs/vgg-{0}'.format(datetime.now().isoformat().replace(':','-').split('.')[0]))]\n",
    "else:\n",
    "    callbacks = []\n",
    "    \n",
    "vgg_based_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/100\n",
      "9199/9199 [==============================] - ETA: 7s - loss: 15.1982 - acc: 0.00 - ETA: 0s - loss: 15.1469 - acc: 0.00 - ETA: 0s - loss: 14.9867 - acc: 0.01 - ETA: 0s - loss: 14.8529 - acc: 0.01 - 1s 61us/step - loss: 14.7239 - acc: 0.0227 - val_loss: 12.2939 - val_acc: 0.0782\n",
      "Epoch 2/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 14.1947 - acc: 0.03 - ETA: 0s - loss: 14.1402 - acc: 0.03 - ETA: 0s - loss: 13.9014 - acc: 0.04 - ETA: 0s - loss: 13.7647 - acc: 0.04 - 0s 23us/step - loss: 13.6788 - acc: 0.0505 - val_loss: 10.4772 - val_acc: 0.1486\n",
      "Epoch 3/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 12.3968 - acc: 0.10 - ETA: 0s - loss: 12.7330 - acc: 0.08 - ETA: 0s - loss: 12.5942 - acc: 0.09 - ETA: 0s - loss: 12.3635 - acc: 0.09 - 0s 21us/step - loss: 12.3028 - acc: 0.0960 - val_loss: 8.4251 - val_acc: 0.2258\n",
      "Epoch 4/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 11.3865 - acc: 0.12 - ETA: 0s - loss: 11.2515 - acc: 0.14 - ETA: 0s - loss: 11.1823 - acc: 0.14 - ETA: 0s - loss: 10.9404 - acc: 0.15 - 0s 20us/step - loss: 10.9231 - acc: 0.1518 - val_loss: 7.2900 - val_acc: 0.3236\n",
      "Epoch 5/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 10.5399 - acc: 0.16 - ETA: 0s - loss: 10.0139 - acc: 0.19 - ETA: 0s - loss: 9.8450 - acc: 0.2049 - 0s 18us/step - loss: 9.6871 - acc: 0.2083 - val_loss: 6.4791 - val_acc: 0.3939\n",
      "Epoch 6/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 8.8628 - acc: 0.234 - ETA: 0s - loss: 9.0480 - acc: 0.251 - ETA: 0s - loss: 8.7961 - acc: 0.256 - ETA: 0s - loss: 8.7351 - acc: 0.257 - 0s 20us/step - loss: 8.6990 - acc: 0.2575 - val_loss: 5.7705 - val_acc: 0.4467\n",
      "Epoch 7/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 8.5333 - acc: 0.312 - ETA: 0s - loss: 8.2289 - acc: 0.289 - ETA: 0s - loss: 8.0575 - acc: 0.291 - 0s 18us/step - loss: 7.9394 - acc: 0.2945 - val_loss: 5.2469 - val_acc: 0.4800\n",
      "Epoch 8/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 7.2327 - acc: 0.343 - ETA: 0s - loss: 7.1239 - acc: 0.341 - ETA: 0s - loss: 7.1526 - acc: 0.338 - 0s 18us/step - loss: 7.1581 - acc: 0.3385 - val_loss: 4.7322 - val_acc: 0.5064\n",
      "Epoch 9/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 6.7242 - acc: 0.363 - ETA: 0s - loss: 6.7421 - acc: 0.357 - ETA: 0s - loss: 6.7806 - acc: 0.357 - 0s 18us/step - loss: 6.6848 - acc: 0.3635 - val_loss: 4.4513 - val_acc: 0.5347\n",
      "Epoch 10/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 6.4098 - acc: 0.386 - ETA: 0s - loss: 6.3040 - acc: 0.402 - ETA: 0s - loss: 6.2279 - acc: 0.403 - ETA: 0s - loss: 6.2250 - acc: 0.402 - 0s 21us/step - loss: 6.1757 - acc: 0.4046 - val_loss: 4.2239 - val_acc: 0.5582\n",
      "Epoch 11/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 6.1777 - acc: 0.429 - ETA: 0s - loss: 5.7783 - acc: 0.433 - ETA: 0s - loss: 5.8771 - acc: 0.420 - ETA: 0s - loss: 5.8119 - acc: 0.424 - 0s 19us/step - loss: 5.8287 - acc: 0.4240 - val_loss: 4.1456 - val_acc: 0.5748\n",
      "Epoch 12/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 5.9233 - acc: 0.418 - ETA: 0s - loss: 5.3713 - acc: 0.455 - ETA: 0s - loss: 5.4442 - acc: 0.448 - ETA: 0s - loss: 5.4478 - acc: 0.449 - 0s 19us/step - loss: 5.4575 - acc: 0.4487 - val_loss: 3.8700 - val_acc: 0.5816\n",
      "Epoch 13/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 5.4089 - acc: 0.441 - ETA: 0s - loss: 5.1540 - acc: 0.468 - ETA: 0s - loss: 5.1919 - acc: 0.465 - 0s 18us/step - loss: 5.1617 - acc: 0.4636 - val_loss: 3.7874 - val_acc: 0.5943\n",
      "Epoch 14/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 4.6125 - acc: 0.500 - ETA: 0s - loss: 4.9501 - acc: 0.476 - ETA: 0s - loss: 4.9140 - acc: 0.478 - 0s 18us/step - loss: 4.8226 - acc: 0.4859 - val_loss: 3.6733 - val_acc: 0.6109\n",
      "Epoch 15/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.8190 - acc: 0.535 - ETA: 0s - loss: 4.5884 - acc: 0.499 - ETA: 0s - loss: 4.6281 - acc: 0.495 - 0s 18us/step - loss: 4.6444 - acc: 0.4960 - val_loss: 3.5099 - val_acc: 0.6051\n",
      "Epoch 16/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 5.1126 - acc: 0.445 - ETA: 0s - loss: 4.4626 - acc: 0.516 - ETA: 0s - loss: 4.4431 - acc: 0.516 - ETA: 0s - loss: 4.4235 - acc: 0.517 - 0s 19us/step - loss: 4.4232 - acc: 0.5172 - val_loss: 3.4714 - val_acc: 0.6168\n",
      "Epoch 17/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 4.0265 - acc: 0.562 - ETA: 0s - loss: 4.1666 - acc: 0.538 - ETA: 0s - loss: 4.2363 - acc: 0.531 - ETA: 0s - loss: 4.2001 - acc: 0.531 - 0s 20us/step - loss: 4.1910 - acc: 0.5321 - val_loss: 3.4686 - val_acc: 0.6188\n",
      "Epoch 18/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.6744 - acc: 0.562 - ETA: 0s - loss: 4.0626 - acc: 0.545 - ETA: 0s - loss: 4.0138 - acc: 0.545 - 0s 18us/step - loss: 4.0914 - acc: 0.5398 - val_loss: 3.4778 - val_acc: 0.6080\n",
      "Epoch 19/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 4.4989 - acc: 0.480 - ETA: 0s - loss: 3.9740 - acc: 0.558 - ETA: 0s - loss: 3.9437 - acc: 0.556 - ETA: 0s - loss: 3.9667 - acc: 0.555 - 0s 19us/step - loss: 3.9678 - acc: 0.5559 - val_loss: 3.4625 - val_acc: 0.6158\n",
      "Epoch 20/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.4423 - acc: 0.554 - ETA: 0s - loss: 3.6647 - acc: 0.570 - ETA: 0s - loss: 3.7557 - acc: 0.566 - ETA: 0s - loss: 3.8849 - acc: 0.559 - 0s 21us/step - loss: 3.9009 - acc: 0.5601 - val_loss: 3.3762 - val_acc: 0.6149\n",
      "Epoch 21/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 4.0026 - acc: 0.574 - ETA: 0s - loss: 3.7341 - acc: 0.570 - ETA: 0s - loss: 3.8055 - acc: 0.565 - ETA: 0s - loss: 3.7367 - acc: 0.570 - 0s 19us/step - loss: 3.7289 - acc: 0.5703 - val_loss: 3.3862 - val_acc: 0.6129\n",
      "Epoch 22/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.6893 - acc: 0.578 - ETA: 0s - loss: 3.7094 - acc: 0.570 - ETA: 0s - loss: 3.7162 - acc: 0.566 - ETA: 0s - loss: 3.7138 - acc: 0.572 - 0s 19us/step - loss: 3.7012 - acc: 0.5730 - val_loss: 3.3378 - val_acc: 0.6227\n",
      "Epoch 23/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.6282 - acc: 0.578 - ETA: 0s - loss: 3.6436 - acc: 0.576 - ETA: 0s - loss: 3.6202 - acc: 0.578 - 0s 18us/step - loss: 3.6234 - acc: 0.5804 - val_loss: 3.2927 - val_acc: 0.6197\n",
      "Epoch 24/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.2760 - acc: 0.601 - ETA: 0s - loss: 3.2806 - acc: 0.593 - ETA: 0s - loss: 3.4225 - acc: 0.590 - 0s 18us/step - loss: 3.4744 - acc: 0.5870 - val_loss: 3.2959 - val_acc: 0.6237\n",
      "Epoch 25/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.7081 - acc: 0.585 - ETA: 0s - loss: 3.3446 - acc: 0.599 - ETA: 0s - loss: 3.3149 - acc: 0.595 - 0s 18us/step - loss: 3.4190 - acc: 0.5897 - val_loss: 3.2417 - val_acc: 0.6178\n",
      "Epoch 26/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.8461 - acc: 0.675 - ETA: 0s - loss: 3.3533 - acc: 0.607 - ETA: 0s - loss: 3.3728 - acc: 0.599 - ETA: 0s - loss: 3.3569 - acc: 0.599 - 0s 19us/step - loss: 3.3607 - acc: 0.5994 - val_loss: 3.2595 - val_acc: 0.6100\n",
      "Epoch 27/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.2711 - acc: 0.582 - ETA: 0s - loss: 3.2033 - acc: 0.609 - ETA: 0s - loss: 3.2434 - acc: 0.611 - ETA: 0s - loss: 3.2800 - acc: 0.608 - 0s 20us/step - loss: 3.2586 - acc: 0.6094 - val_loss: 3.2059 - val_acc: 0.6139\n",
      "Epoch 28/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.4064 - acc: 0.597 - ETA: 0s - loss: 3.3604 - acc: 0.608 - ETA: 0s - loss: 3.2669 - acc: 0.612 - ETA: 0s - loss: 3.2256 - acc: 0.608 - 0s 19us/step - loss: 3.2156 - acc: 0.6091 - val_loss: 3.1454 - val_acc: 0.6237\n",
      "Epoch 29/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.1103 - acc: 0.640 - ETA: 0s - loss: 3.2807 - acc: 0.603 - ETA: 0s - loss: 3.1978 - acc: 0.613 - ETA: 0s - loss: 3.1653 - acc: 0.617 - 0s 19us/step - loss: 3.1469 - acc: 0.6187 - val_loss: 3.1000 - val_acc: 0.6315\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - ETA: 0s - loss: 2.7909 - acc: 0.632 - ETA: 0s - loss: 3.0820 - acc: 0.627 - ETA: 0s - loss: 3.0570 - acc: 0.621 - ETA: 0s - loss: 3.0938 - acc: 0.618 - 0s 20us/step - loss: 3.0779 - acc: 0.6195 - val_loss: 3.1362 - val_acc: 0.6237\n",
      "Epoch 31/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.0640 - acc: 0.589 - ETA: 0s - loss: 3.0543 - acc: 0.615 - ETA: 0s - loss: 3.0276 - acc: 0.619 - 0s 18us/step - loss: 3.0667 - acc: 0.6169 - val_loss: 3.1216 - val_acc: 0.6227\n",
      "Epoch 32/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.0687 - acc: 0.648 - ETA: 0s - loss: 2.9339 - acc: 0.637 - ETA: 0s - loss: 2.9550 - acc: 0.629 - 0s 18us/step - loss: 2.9896 - acc: 0.6266 - val_loss: 3.0545 - val_acc: 0.6285\n",
      "Epoch 33/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.6990 - acc: 0.636 - ETA: 0s - loss: 2.8583 - acc: 0.634 - ETA: 0s - loss: 2.8599 - acc: 0.638 - 0s 18us/step - loss: 2.9627 - acc: 0.6292 - val_loss: 2.9996 - val_acc: 0.6266\n",
      "Epoch 34/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.6454 - acc: 0.574 - ETA: 0s - loss: 2.9618 - acc: 0.618 - ETA: 0s - loss: 2.9597 - acc: 0.620 - 0s 18us/step - loss: 2.9723 - acc: 0.6196 - val_loss: 3.0467 - val_acc: 0.6334\n",
      "Epoch 35/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.2144 - acc: 0.625 - ETA: 0s - loss: 2.8942 - acc: 0.630 - ETA: 0s - loss: 2.9308 - acc: 0.629 - 0s 18us/step - loss: 2.9302 - acc: 0.6292 - val_loss: 3.0270 - val_acc: 0.6276\n",
      "Epoch 36/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.7331 - acc: 0.625 - ETA: 0s - loss: 2.7589 - acc: 0.639 - ETA: 0s - loss: 2.8224 - acc: 0.638 - 0s 18us/step - loss: 2.8507 - acc: 0.6360 - val_loss: 2.9843 - val_acc: 0.6178\n",
      "Epoch 37/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.0043 - acc: 0.660 - ETA: 0s - loss: 2.7173 - acc: 0.650 - ETA: 0s - loss: 2.8015 - acc: 0.644 - 0s 17us/step - loss: 2.7941 - acc: 0.6422 - val_loss: 2.8697 - val_acc: 0.6197\n",
      "Epoch 38/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.3114 - acc: 0.582 - ETA: 0s - loss: 2.6883 - acc: 0.647 - ETA: 0s - loss: 2.6744 - acc: 0.650 - 0s 18us/step - loss: 2.7333 - acc: 0.6426 - val_loss: 2.8499 - val_acc: 0.6246\n",
      "Epoch 39/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2091 - acc: 0.683 - ETA: 0s - loss: 2.6927 - acc: 0.651 - ETA: 0s - loss: 2.7264 - acc: 0.648 - 0s 18us/step - loss: 2.6913 - acc: 0.6499 - val_loss: 2.8589 - val_acc: 0.6334\n",
      "Epoch 40/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.7221 - acc: 0.609 - ETA: 0s - loss: 2.6475 - acc: 0.647 - ETA: 0s - loss: 2.6669 - acc: 0.642 - ETA: 0s - loss: 2.7086 - acc: 0.641 - 0s 19us/step - loss: 2.7128 - acc: 0.6409 - val_loss: 2.8470 - val_acc: 0.6364\n",
      "Epoch 41/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.5103 - acc: 0.621 - ETA: 0s - loss: 2.6859 - acc: 0.650 - ETA: 0s - loss: 2.6795 - acc: 0.649 - 0s 18us/step - loss: 2.6819 - acc: 0.6499 - val_loss: 2.8903 - val_acc: 0.6276\n",
      "Epoch 42/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4329 - acc: 0.710 - ETA: 0s - loss: 2.6571 - acc: 0.661 - ETA: 0s - loss: 2.6785 - acc: 0.655 - 0s 18us/step - loss: 2.6358 - acc: 0.6565 - val_loss: 2.8678 - val_acc: 0.6344\n",
      "Epoch 43/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2151 - acc: 0.695 - ETA: 0s - loss: 2.6431 - acc: 0.662 - ETA: 0s - loss: 2.5901 - acc: 0.658 - 0s 18us/step - loss: 2.6023 - acc: 0.6549 - val_loss: 2.8560 - val_acc: 0.6246\n",
      "Epoch 44/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2225 - acc: 0.703 - ETA: 0s - loss: 2.6612 - acc: 0.654 - ETA: 0s - loss: 2.5618 - acc: 0.660 - 0s 18us/step - loss: 2.5984 - acc: 0.6502 - val_loss: 2.9091 - val_acc: 0.6276\n",
      "Epoch 45/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2966 - acc: 0.644 - ETA: 0s - loss: 2.5022 - acc: 0.664 - ETA: 0s - loss: 2.5756 - acc: 0.652 - ETA: 0s - loss: 2.6188 - acc: 0.651 - 0s 21us/step - loss: 2.6292 - acc: 0.6514 - val_loss: 2.9004 - val_acc: 0.6295\n",
      "Epoch 46/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.9049 - acc: 0.648 - ETA: 0s - loss: 2.6642 - acc: 0.657 - ETA: 0s - loss: 2.6079 - acc: 0.658 - ETA: 0s - loss: 2.5437 - acc: 0.662 - 0s 19us/step - loss: 2.5483 - acc: 0.6626 - val_loss: 2.8110 - val_acc: 0.6432\n",
      "Epoch 47/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.5991 - acc: 0.660 - ETA: 0s - loss: 2.4474 - acc: 0.667 - ETA: 0s - loss: 2.5157 - acc: 0.663 - 0s 18us/step - loss: 2.5575 - acc: 0.6594 - val_loss: 2.8051 - val_acc: 0.6364\n",
      "Epoch 48/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 3.0869 - acc: 0.609 - ETA: 0s - loss: 2.5093 - acc: 0.660 - ETA: 0s - loss: 2.5608 - acc: 0.654 - 0s 18us/step - loss: 2.5671 - acc: 0.6530 - val_loss: 2.8072 - val_acc: 0.6227\n",
      "Epoch 49/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4942 - acc: 0.660 - ETA: 0s - loss: 2.4445 - acc: 0.667 - ETA: 0s - loss: 2.4842 - acc: 0.661 - ETA: 0s - loss: 2.5424 - acc: 0.658 - 0s 20us/step - loss: 2.5501 - acc: 0.6569 - val_loss: 2.8138 - val_acc: 0.6256\n",
      "Epoch 50/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.1654 - acc: 0.664 - ETA: 0s - loss: 2.6386 - acc: 0.648 - ETA: 0s - loss: 2.5242 - acc: 0.657 - 0s 18us/step - loss: 2.5253 - acc: 0.6615 - val_loss: 2.8165 - val_acc: 0.6334\n",
      "Epoch 51/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2467 - acc: 0.707 - ETA: 0s - loss: 2.5212 - acc: 0.660 - ETA: 0s - loss: 2.4936 - acc: 0.657 - ETA: 0s - loss: 2.4947 - acc: 0.656 - 0s 19us/step - loss: 2.4992 - acc: 0.6560 - val_loss: 2.6481 - val_acc: 0.6334\n",
      "Epoch 52/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.7001 - acc: 0.652 - ETA: 0s - loss: 2.5454 - acc: 0.660 - ETA: 0s - loss: 2.4560 - acc: 0.660 - ETA: 0s - loss: 2.4917 - acc: 0.661 - 0s 20us/step - loss: 2.4811 - acc: 0.6615 - val_loss: 2.6582 - val_acc: 0.6354\n",
      "Epoch 53/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.8262 - acc: 0.601 - ETA: 0s - loss: 2.3958 - acc: 0.669 - ETA: 0s - loss: 2.2973 - acc: 0.676 - 0s 18us/step - loss: 2.3936 - acc: 0.6730 - val_loss: 2.5999 - val_acc: 0.6452\n",
      "Epoch 54/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4396 - acc: 0.703 - ETA: 0s - loss: 2.3930 - acc: 0.666 - ETA: 0s - loss: 2.3662 - acc: 0.669 - 0s 18us/step - loss: 2.4086 - acc: 0.6674 - val_loss: 2.6084 - val_acc: 0.6491\n",
      "Epoch 55/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2001 - acc: 0.710 - ETA: 0s - loss: 2.4061 - acc: 0.671 - ETA: 0s - loss: 2.3938 - acc: 0.667 - 0s 18us/step - loss: 2.4067 - acc: 0.6630 - val_loss: 2.7090 - val_acc: 0.6315\n",
      "Epoch 56/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.8773 - acc: 0.703 - ETA: 0s - loss: 2.4756 - acc: 0.658 - ETA: 0s - loss: 2.4307 - acc: 0.666 - ETA: 0s - loss: 2.4068 - acc: 0.668 - 0s 19us/step - loss: 2.3926 - acc: 0.6689 - val_loss: 2.7114 - val_acc: 0.6227\n",
      "Epoch 57/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.8184 - acc: 0.726 - ETA: 0s - loss: 2.4347 - acc: 0.663 - ETA: 0s - loss: 2.3628 - acc: 0.672 - ETA: 0s - loss: 2.3741 - acc: 0.669 - 0s 21us/step - loss: 2.3759 - acc: 0.6687 - val_loss: 2.6835 - val_acc: 0.6344\n",
      "Epoch 58/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4453 - acc: 0.668 - ETA: 0s - loss: 2.3864 - acc: 0.667 - ETA: 0s - loss: 2.3397 - acc: 0.670 - 0s 18us/step - loss: 2.3543 - acc: 0.6691 - val_loss: 2.6193 - val_acc: 0.6373\n",
      "Epoch 59/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4594 - acc: 0.675 - ETA: 0s - loss: 2.3813 - acc: 0.662 - ETA: 0s - loss: 2.3379 - acc: 0.665 - 0s 18us/step - loss: 2.3846 - acc: 0.6629 - val_loss: 2.6129 - val_acc: 0.6432\n",
      "Epoch 60/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.8683 - acc: 0.730 - ETA: 0s - loss: 2.2188 - acc: 0.681 - ETA: 0s - loss: 2.2838 - acc: 0.678 - 0s 18us/step - loss: 2.3250 - acc: 0.6736 - val_loss: 2.6409 - val_acc: 0.6266\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - ETA: 0s - loss: 2.0614 - acc: 0.683 - ETA: 0s - loss: 2.2440 - acc: 0.685 - ETA: 0s - loss: 2.2629 - acc: 0.678 - 0s 18us/step - loss: 2.3308 - acc: 0.6724 - val_loss: 2.6341 - val_acc: 0.6354\n",
      "Epoch 62/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2643 - acc: 0.695 - ETA: 0s - loss: 2.2808 - acc: 0.678 - ETA: 0s - loss: 2.2833 - acc: 0.677 - 0s 18us/step - loss: 2.3169 - acc: 0.6721 - val_loss: 2.6589 - val_acc: 0.6344\n",
      "Epoch 63/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.3399 - acc: 0.699 - ETA: 0s - loss: 2.2645 - acc: 0.682 - ETA: 0s - loss: 2.3611 - acc: 0.672 - ETA: 0s - loss: 2.3235 - acc: 0.677 - 0s 19us/step - loss: 2.3263 - acc: 0.6770 - val_loss: 2.6594 - val_acc: 0.6364\n",
      "Epoch 64/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.0796 - acc: 0.707 - ETA: 0s - loss: 2.2042 - acc: 0.692 - ETA: 0s - loss: 2.2506 - acc: 0.682 - 0s 17us/step - loss: 2.3089 - acc: 0.6751 - val_loss: 2.6684 - val_acc: 0.6276\n",
      "Epoch 65/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2798 - acc: 0.664 - ETA: 0s - loss: 2.3984 - acc: 0.655 - ETA: 0s - loss: 2.3356 - acc: 0.660 - 0s 18us/step - loss: 2.3327 - acc: 0.6597 - val_loss: 2.6879 - val_acc: 0.6178\n",
      "Epoch 66/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.1683 - acc: 0.691 - ETA: 0s - loss: 2.3817 - acc: 0.667 - ETA: 0s - loss: 2.3382 - acc: 0.678 - 0s 18us/step - loss: 2.3222 - acc: 0.6778 - val_loss: 2.6582 - val_acc: 0.6373\n",
      "Epoch 67/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2915 - acc: 0.664 - ETA: 0s - loss: 2.2051 - acc: 0.685 - ETA: 0s - loss: 2.2676 - acc: 0.675 - 0s 18us/step - loss: 2.3006 - acc: 0.6714 - val_loss: 2.7061 - val_acc: 0.6207\n",
      "Epoch 68/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.6248 - acc: 0.656 - ETA: 0s - loss: 2.3403 - acc: 0.666 - ETA: 0s - loss: 2.3095 - acc: 0.671 - 0s 18us/step - loss: 2.3477 - acc: 0.6664 - val_loss: 2.6759 - val_acc: 0.6188\n",
      "Epoch 69/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4187 - acc: 0.652 - ETA: 0s - loss: 2.2269 - acc: 0.681 - ETA: 0s - loss: 2.2473 - acc: 0.678 - ETA: 0s - loss: 2.2662 - acc: 0.677 - 0s 19us/step - loss: 2.2745 - acc: 0.6770 - val_loss: 2.6828 - val_acc: 0.6315\n",
      "Epoch 70/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.9891 - acc: 0.691 - ETA: 0s - loss: 2.1941 - acc: 0.692 - ETA: 0s - loss: 2.2538 - acc: 0.678 - 0s 17us/step - loss: 2.2753 - acc: 0.6762 - val_loss: 2.6618 - val_acc: 0.6266\n",
      "Epoch 71/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.3121 - acc: 0.664 - ETA: 0s - loss: 2.2226 - acc: 0.679 - ETA: 0s - loss: 2.2096 - acc: 0.681 - 0s 18us/step - loss: 2.2786 - acc: 0.6738 - val_loss: 2.6508 - val_acc: 0.6305\n",
      "Epoch 72/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.0076 - acc: 0.707 - ETA: 0s - loss: 2.2101 - acc: 0.685 - ETA: 0s - loss: 2.2712 - acc: 0.676 - 0s 18us/step - loss: 2.2743 - acc: 0.6745 - val_loss: 2.6374 - val_acc: 0.6266\n",
      "Epoch 73/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.5827 - acc: 0.761 - ETA: 0s - loss: 2.1878 - acc: 0.692 - ETA: 0s - loss: 2.2419 - acc: 0.683 - 0s 18us/step - loss: 2.2539 - acc: 0.6794 - val_loss: 2.7086 - val_acc: 0.6149\n",
      "Epoch 74/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.7597 - acc: 0.652 - ETA: 0s - loss: 2.3380 - acc: 0.681 - ETA: 0s - loss: 2.2900 - acc: 0.672 - ETA: 0s - loss: 2.2637 - acc: 0.671 - 0s 20us/step - loss: 2.2767 - acc: 0.6705 - val_loss: 2.7228 - val_acc: 0.6158\n",
      "Epoch 75/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.3082 - acc: 0.695 - ETA: 0s - loss: 2.2497 - acc: 0.680 - ETA: 0s - loss: 2.2741 - acc: 0.676 - ETA: 0s - loss: 2.2856 - acc: 0.676 - 0s 19us/step - loss: 2.2899 - acc: 0.6765 - val_loss: 2.6556 - val_acc: 0.6344\n",
      "Epoch 76/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.0310 - acc: 0.707 - ETA: 0s - loss: 2.2441 - acc: 0.677 - ETA: 0s - loss: 2.2275 - acc: 0.677 - 0s 18us/step - loss: 2.2566 - acc: 0.6751 - val_loss: 2.6447 - val_acc: 0.6325\n",
      "Epoch 77/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.8375 - acc: 0.710 - ETA: 0s - loss: 2.2689 - acc: 0.674 - ETA: 0s - loss: 2.2638 - acc: 0.675 - ETA: 0s - loss: 2.2723 - acc: 0.674 - 0s 19us/step - loss: 2.2812 - acc: 0.6724 - val_loss: 2.6793 - val_acc: 0.6305\n",
      "Epoch 78/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.1779 - acc: 0.683 - ETA: 0s - loss: 2.2526 - acc: 0.677 - ETA: 0s - loss: 2.2503 - acc: 0.681 - ETA: 0s - loss: 2.2261 - acc: 0.679 - 0s 20us/step - loss: 2.2379 - acc: 0.6779 - val_loss: 2.6141 - val_acc: 0.6432\n",
      "Epoch 79/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.1369 - acc: 0.695 - ETA: 0s - loss: 2.1509 - acc: 0.688 - ETA: 0s - loss: 2.1926 - acc: 0.684 - 0s 18us/step - loss: 2.2438 - acc: 0.6758 - val_loss: 2.6221 - val_acc: 0.6422\n",
      "Epoch 80/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.7419 - acc: 0.765 - ETA: 0s - loss: 2.1764 - acc: 0.678 - ETA: 0s - loss: 2.2395 - acc: 0.672 - ETA: 0s - loss: 2.2689 - acc: 0.670 - 0s 19us/step - loss: 2.2606 - acc: 0.6720 - val_loss: 2.6493 - val_acc: 0.6276\n",
      "Epoch 81/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.1909 - acc: 0.671 - ETA: 0s - loss: 2.2226 - acc: 0.689 - ETA: 0s - loss: 2.2125 - acc: 0.685 - ETA: 0s - loss: 2.2704 - acc: 0.678 - 0s 19us/step - loss: 2.2683 - acc: 0.6779 - val_loss: 2.6719 - val_acc: 0.6266\n",
      "Epoch 82/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.5135 - acc: 0.675 - ETA: 0s - loss: 2.2952 - acc: 0.659 - ETA: 0s - loss: 2.3107 - acc: 0.660 - 0s 18us/step - loss: 2.3223 - acc: 0.6624 - val_loss: 2.6999 - val_acc: 0.6295\n",
      "Epoch 83/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.6027 - acc: 0.640 - ETA: 0s - loss: 2.3672 - acc: 0.671 - ETA: 0s - loss: 2.3056 - acc: 0.678 - 0s 18us/step - loss: 2.2991 - acc: 0.6727 - val_loss: 2.6401 - val_acc: 0.6315\n",
      "Epoch 84/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.5238 - acc: 0.656 - ETA: 0s - loss: 2.2137 - acc: 0.681 - ETA: 0s - loss: 2.3002 - acc: 0.667 - 0s 17us/step - loss: 2.2648 - acc: 0.6701 - val_loss: 2.6096 - val_acc: 0.6393\n",
      "Epoch 85/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.3299 - acc: 0.671 - ETA: 0s - loss: 2.1817 - acc: 0.683 - ETA: 0s - loss: 2.2309 - acc: 0.671 - 0s 18us/step - loss: 2.2798 - acc: 0.6684 - val_loss: 2.6714 - val_acc: 0.6276\n",
      "Epoch 86/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.9224 - acc: 0.679 - ETA: 0s - loss: 2.3060 - acc: 0.685 - ETA: 0s - loss: 2.2573 - acc: 0.683 - 0s 18us/step - loss: 2.2903 - acc: 0.6775 - val_loss: 2.6817 - val_acc: 0.6256\n",
      "Epoch 87/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.3537 - acc: 0.640 - ETA: 0s - loss: 2.2900 - acc: 0.672 - ETA: 0s - loss: 2.2129 - acc: 0.681 - ETA: 0s - loss: 2.2567 - acc: 0.677 - 0s 19us/step - loss: 2.2657 - acc: 0.6777 - val_loss: 2.6681 - val_acc: 0.6256\n",
      "Epoch 88/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.0452 - acc: 0.707 - ETA: 0s - loss: 2.0458 - acc: 0.697 - ETA: 0s - loss: 2.2081 - acc: 0.690 - 0s 18us/step - loss: 2.2313 - acc: 0.6812 - val_loss: 2.6674 - val_acc: 0.6344\n",
      "Epoch 89/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.3691 - acc: 0.695 - ETA: 0s - loss: 2.2571 - acc: 0.681 - ETA: 0s - loss: 2.2079 - acc: 0.685 - 0s 18us/step - loss: 2.2160 - acc: 0.6830 - val_loss: 2.6316 - val_acc: 0.6383\n",
      "Epoch 90/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2130 - acc: 0.660 - ETA: 0s - loss: 2.2215 - acc: 0.682 - ETA: 0s - loss: 2.2620 - acc: 0.672 - 0s 18us/step - loss: 2.2800 - acc: 0.6704 - val_loss: 2.6603 - val_acc: 0.6237\n",
      "Epoch 91/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4399 - acc: 0.664 - ETA: 0s - loss: 2.1738 - acc: 0.680 - ETA: 0s - loss: 2.2050 - acc: 0.676 - 0s 18us/step - loss: 2.2591 - acc: 0.6699 - val_loss: 2.6555 - val_acc: 0.6364\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4521 - acc: 0.679 - ETA: 0s - loss: 2.1849 - acc: 0.690 - ETA: 0s - loss: 2.2339 - acc: 0.680 - 0s 18us/step - loss: 2.2352 - acc: 0.6795 - val_loss: 2.7022 - val_acc: 0.6315\n",
      "Epoch 93/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4643 - acc: 0.636 - ETA: 0s - loss: 2.1544 - acc: 0.684 - ETA: 0s - loss: 2.2636 - acc: 0.678 - 0s 18us/step - loss: 2.2372 - acc: 0.6772 - val_loss: 2.6316 - val_acc: 0.6383\n",
      "Epoch 94/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4090 - acc: 0.644 - ETA: 0s - loss: 2.2224 - acc: 0.684 - ETA: 0s - loss: 2.2350 - acc: 0.679 - 0s 18us/step - loss: 2.2364 - acc: 0.6780 - val_loss: 2.6584 - val_acc: 0.6354\n",
      "Epoch 95/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4846 - acc: 0.664 - ETA: 0s - loss: 2.1503 - acc: 0.694 - ETA: 0s - loss: 2.1932 - acc: 0.682 - 0s 18us/step - loss: 2.2367 - acc: 0.6791 - val_loss: 2.6118 - val_acc: 0.6237\n",
      "Epoch 96/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.7711 - acc: 0.722 - ETA: 0s - loss: 2.1573 - acc: 0.685 - ETA: 0s - loss: 2.1927 - acc: 0.684 - 0s 18us/step - loss: 2.2265 - acc: 0.6819 - val_loss: 2.7441 - val_acc: 0.6129\n",
      "Epoch 97/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2191 - acc: 0.687 - ETA: 0s - loss: 2.2119 - acc: 0.680 - ETA: 0s - loss: 2.2207 - acc: 0.675 - 0s 18us/step - loss: 2.2515 - acc: 0.6752 - val_loss: 2.6991 - val_acc: 0.6207\n",
      "Epoch 98/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.5091 - acc: 0.652 - ETA: 0s - loss: 2.1671 - acc: 0.682 - ETA: 0s - loss: 2.2049 - acc: 0.679 - 0s 18us/step - loss: 2.2562 - acc: 0.6766 - val_loss: 2.6966 - val_acc: 0.6168\n",
      "Epoch 99/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.4366 - acc: 0.691 - ETA: 0s - loss: 2.2947 - acc: 0.682 - ETA: 0s - loss: 2.2473 - acc: 0.678 - 0s 18us/step - loss: 2.2669 - acc: 0.6745 - val_loss: 2.7248 - val_acc: 0.6109\n",
      "Epoch 100/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.0381 - acc: 0.703 - ETA: 0s - loss: 2.1325 - acc: 0.687 - ETA: 0s - loss: 2.2474 - acc: 0.674 - 0s 18us/step - loss: 2.2638 - acc: 0.6709 - val_loss: 2.7089 - val_acc: 0.6207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca4d084390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_based_model.fit(vgg_features, y_train, batch_size=256, epochs=EPOCHS, callbacks=callbacks, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_features = resnet50_features_extractor.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_based_model = create_simple_model(num_classes, resnet50_features_extractor.model.output.get_shape().as_list()[1:])\n",
    "resnet50_based_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "if NOT_KAGGLE_KERNEL:\n",
    "    callbacks = [TensorBoard('./data/logs/resnet50-{0}'.format(datetime.now().isoformat().replace(':','-').split('.')[0]))]\n",
    "else:\n",
    "    callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/100\n",
      "9199/9199 [==============================] - ETA: 7s - loss: 6.1339 - acc: 0.007 - ETA: 0s - loss: 5.5648 - acc: 0.023 - ETA: 0s - loss: 4.9517 - acc: 0.059 - ETA: 0s - loss: 4.4822 - acc: 0.100 - ETA: 0s - loss: 4.0715 - acc: 0.152 - 1s 66us/step - loss: 3.9010 - acc: 0.1751 - val_loss: 1.7492 - val_acc: 0.5787\n",
      "Epoch 2/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 2.2155 - acc: 0.457 - ETA: 0s - loss: 2.0251 - acc: 0.478 - ETA: 0s - loss: 1.8951 - acc: 0.502 - ETA: 0s - loss: 1.8119 - acc: 0.515 - ETA: 0s - loss: 1.7327 - acc: 0.534 - 0s 28us/step - loss: 1.7052 - acc: 0.5399 - val_loss: 1.1239 - val_acc: 0.6940\n",
      "Epoch 3/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 1.5109 - acc: 0.593 - ETA: 0s - loss: 1.3013 - acc: 0.636 - ETA: 0s - loss: 1.2573 - acc: 0.646 - ETA: 0s - loss: 1.2225 - acc: 0.652 - ETA: 0s - loss: 1.1927 - acc: 0.660 - 0s 27us/step - loss: 1.1831 - acc: 0.6626 - val_loss: 0.9474 - val_acc: 0.7341\n",
      "Epoch 4/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.8646 - acc: 0.757 - ETA: 0s - loss: 0.9769 - acc: 0.718 - ETA: 0s - loss: 0.9742 - acc: 0.716 - ETA: 0s - loss: 0.9545 - acc: 0.722 - ETA: 0s - loss: 0.9561 - acc: 0.720 - 0s 28us/step - loss: 0.9456 - acc: 0.7243 - val_loss: 0.8688 - val_acc: 0.7468\n",
      "Epoch 5/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.8157 - acc: 0.750 - ETA: 0s - loss: 0.8359 - acc: 0.751 - ETA: 0s - loss: 0.8184 - acc: 0.756 - ETA: 0s - loss: 0.8108 - acc: 0.758 - ETA: 0s - loss: 0.8018 - acc: 0.761 - 0s 28us/step - loss: 0.8008 - acc: 0.7614 - val_loss: 0.8345 - val_acc: 0.7400\n",
      "Epoch 6/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.7957 - acc: 0.773 - ETA: 0s - loss: 0.6715 - acc: 0.798 - ETA: 0s - loss: 0.6783 - acc: 0.798 - ETA: 0s - loss: 0.6855 - acc: 0.796 - ETA: 0s - loss: 0.6896 - acc: 0.796 - 0s 27us/step - loss: 0.6877 - acc: 0.7967 - val_loss: 0.7934 - val_acc: 0.7478\n",
      "Epoch 7/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.5958 - acc: 0.820 - ETA: 0s - loss: 0.5893 - acc: 0.824 - ETA: 0s - loss: 0.5960 - acc: 0.823 - ETA: 0s - loss: 0.5994 - acc: 0.822 - ETA: 0s - loss: 0.6045 - acc: 0.821 - 0s 27us/step - loss: 0.6026 - acc: 0.8223 - val_loss: 0.7879 - val_acc: 0.7546\n",
      "Epoch 8/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.5578 - acc: 0.843 - ETA: 0s - loss: 0.5511 - acc: 0.833 - ETA: 0s - loss: 0.5410 - acc: 0.837 - ETA: 0s - loss: 0.5354 - acc: 0.839 - ETA: 0s - loss: 0.5371 - acc: 0.836 - 0s 28us/step - loss: 0.5399 - acc: 0.8361 - val_loss: 0.7639 - val_acc: 0.7527\n",
      "Epoch 9/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.4480 - acc: 0.875 - ETA: 0s - loss: 0.4492 - acc: 0.868 - ETA: 0s - loss: 0.4624 - acc: 0.861 - ETA: 0s - loss: 0.4670 - acc: 0.861 - ETA: 0s - loss: 0.4729 - acc: 0.859 - 0s 27us/step - loss: 0.4766 - acc: 0.8582 - val_loss: 0.7645 - val_acc: 0.7703\n",
      "Epoch 10/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.4249 - acc: 0.867 - ETA: 0s - loss: 0.4428 - acc: 0.871 - ETA: 0s - loss: 0.4362 - acc: 0.875 - ETA: 0s - loss: 0.4360 - acc: 0.875 - ETA: 0s - loss: 0.4426 - acc: 0.872 - 0s 27us/step - loss: 0.4406 - acc: 0.8738 - val_loss: 0.7300 - val_acc: 0.7674\n",
      "Epoch 11/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.3479 - acc: 0.875 - ETA: 0s - loss: 0.3771 - acc: 0.882 - ETA: 0s - loss: 0.3855 - acc: 0.888 - ETA: 0s - loss: 0.3887 - acc: 0.886 - ETA: 0s - loss: 0.3900 - acc: 0.887 - 0s 27us/step - loss: 0.3946 - acc: 0.8848 - val_loss: 0.7572 - val_acc: 0.7761\n",
      "Epoch 12/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.3643 - acc: 0.902 - ETA: 0s - loss: 0.3711 - acc: 0.890 - ETA: 0s - loss: 0.3668 - acc: 0.894 - ETA: 0s - loss: 0.3636 - acc: 0.894 - ETA: 0s - loss: 0.3647 - acc: 0.894 - 0s 27us/step - loss: 0.3682 - acc: 0.8926 - val_loss: 0.7348 - val_acc: 0.7674\n",
      "Epoch 13/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2722 - acc: 0.929 - ETA: 0s - loss: 0.3334 - acc: 0.906 - ETA: 0s - loss: 0.3456 - acc: 0.900 - ETA: 0s - loss: 0.3393 - acc: 0.903 - ETA: 0s - loss: 0.3366 - acc: 0.904 - 0s 27us/step - loss: 0.3376 - acc: 0.9044 - val_loss: 0.7335 - val_acc: 0.7674\n",
      "Epoch 14/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.3545 - acc: 0.894 - ETA: 0s - loss: 0.3196 - acc: 0.911 - ETA: 0s - loss: 0.3110 - acc: 0.912 - ETA: 0s - loss: 0.3194 - acc: 0.909 - ETA: 0s - loss: 0.3178 - acc: 0.910 - 0s 27us/step - loss: 0.3213 - acc: 0.9093 - val_loss: 0.7311 - val_acc: 0.7693\n",
      "Epoch 15/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.3896 - acc: 0.890 - ETA: 0s - loss: 0.2936 - acc: 0.921 - ETA: 0s - loss: 0.2999 - acc: 0.917 - ETA: 0s - loss: 0.2949 - acc: 0.919 - ETA: 0s - loss: 0.2956 - acc: 0.917 - 0s 28us/step - loss: 0.2969 - acc: 0.9173 - val_loss: 0.7350 - val_acc: 0.7703\n",
      "Epoch 16/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2552 - acc: 0.937 - ETA: 0s - loss: 0.2739 - acc: 0.922 - ETA: 0s - loss: 0.2670 - acc: 0.926 - ETA: 0s - loss: 0.2719 - acc: 0.924 - ETA: 0s - loss: 0.2776 - acc: 0.921 - 0s 28us/step - loss: 0.2785 - acc: 0.9201 - val_loss: 0.7229 - val_acc: 0.7752\n",
      "Epoch 17/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.3361 - acc: 0.918 - ETA: 0s - loss: 0.2858 - acc: 0.918 - ETA: 0s - loss: 0.2658 - acc: 0.923 - ETA: 0s - loss: 0.2686 - acc: 0.924 - ETA: 0s - loss: 0.2691 - acc: 0.925 - 0s 28us/step - loss: 0.2677 - acc: 0.9259 - val_loss: 0.7319 - val_acc: 0.7742\n",
      "Epoch 18/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2290 - acc: 0.937 - ETA: 0s - loss: 0.2480 - acc: 0.930 - ETA: 0s - loss: 0.2538 - acc: 0.928 - ETA: 0s - loss: 0.2551 - acc: 0.927 - ETA: 0s - loss: 0.2542 - acc: 0.928 - 0s 27us/step - loss: 0.2547 - acc: 0.9280 - val_loss: 0.7369 - val_acc: 0.7742\n",
      "Epoch 19/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2474 - acc: 0.918 - ETA: 0s - loss: 0.2324 - acc: 0.939 - ETA: 0s - loss: 0.2411 - acc: 0.936 - ETA: 0s - loss: 0.2413 - acc: 0.937 - ETA: 0s - loss: 0.2388 - acc: 0.938 - 0s 28us/step - loss: 0.2394 - acc: 0.9392 - val_loss: 0.7326 - val_acc: 0.7742\n",
      "Epoch 20/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2536 - acc: 0.937 - ETA: 0s - loss: 0.2144 - acc: 0.948 - ETA: 0s - loss: 0.2157 - acc: 0.946 - ETA: 0s - loss: 0.2170 - acc: 0.943 - ETA: 0s - loss: 0.2189 - acc: 0.940 - 0s 28us/step - loss: 0.2180 - acc: 0.9408 - val_loss: 0.7323 - val_acc: 0.7801\n",
      "Epoch 21/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.949 - ETA: 0s - loss: 0.2134 - acc: 0.943 - ETA: 0s - loss: 0.2044 - acc: 0.945 - ETA: 0s - loss: 0.2126 - acc: 0.938 - ETA: 0s - loss: 0.2138 - acc: 0.938 - 0s 28us/step - loss: 0.2152 - acc: 0.9375 - val_loss: 0.7339 - val_acc: 0.7761\n",
      "Epoch 22/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1752 - acc: 0.960 - ETA: 0s - loss: 0.1943 - acc: 0.950 - ETA: 0s - loss: 0.2027 - acc: 0.949 - ETA: 0s - loss: 0.2013 - acc: 0.950 - ETA: 0s - loss: 0.2026 - acc: 0.949 - 0s 28us/step - loss: 0.2023 - acc: 0.9481 - val_loss: 0.7338 - val_acc: 0.7752\n",
      "Epoch 23/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1603 - acc: 0.968 - ETA: 0s - loss: 0.1722 - acc: 0.954 - ETA: 0s - loss: 0.1871 - acc: 0.950 - ETA: 0s - loss: 0.1890 - acc: 0.950 - ETA: 0s - loss: 0.1915 - acc: 0.950 - 0s 29us/step - loss: 0.1921 - acc: 0.9500 - val_loss: 0.7409 - val_acc: 0.7810\n",
      "Epoch 24/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2496 - acc: 0.902 - ETA: 0s - loss: 0.1812 - acc: 0.954 - ETA: 0s - loss: 0.1829 - acc: 0.953 - ETA: 0s - loss: 0.1860 - acc: 0.952 - ETA: 0s - loss: 0.1867 - acc: 0.952 - 0s 28us/step - loss: 0.1862 - acc: 0.9523 - val_loss: 0.7361 - val_acc: 0.7771\n",
      "Epoch 25/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1921 - acc: 0.941 - ETA: 0s - loss: 0.1752 - acc: 0.956 - ETA: 0s - loss: 0.1774 - acc: 0.954 - ETA: 0s - loss: 0.1739 - acc: 0.954 - ETA: 0s - loss: 0.1745 - acc: 0.953 - 0s 28us/step - loss: 0.1750 - acc: 0.9539 - val_loss: 0.7536 - val_acc: 0.7752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.2022 - acc: 0.945 - ETA: 0s - loss: 0.1663 - acc: 0.960 - ETA: 0s - loss: 0.1628 - acc: 0.959 - ETA: 0s - loss: 0.1629 - acc: 0.959 - ETA: 0s - loss: 0.1611 - acc: 0.960 - 0s 27us/step - loss: 0.1620 - acc: 0.9595 - val_loss: 0.7380 - val_acc: 0.7761\n",
      "Epoch 27/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1562 - acc: 0.960 - ETA: 0s - loss: 0.1536 - acc: 0.967 - ETA: 0s - loss: 0.1623 - acc: 0.959 - ETA: 0s - loss: 0.1614 - acc: 0.960 - ETA: 0s - loss: 0.1623 - acc: 0.959 - 0s 28us/step - loss: 0.1622 - acc: 0.9596 - val_loss: 0.7555 - val_acc: 0.7654\n",
      "Epoch 28/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1313 - acc: 0.968 - ETA: 0s - loss: 0.1474 - acc: 0.962 - ETA: 0s - loss: 0.1451 - acc: 0.960 - ETA: 0s - loss: 0.1517 - acc: 0.958 - 0s 26us/step - loss: 0.1560 - acc: 0.9579 - val_loss: 0.7482 - val_acc: 0.7703\n",
      "Epoch 29/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1523 - acc: 0.972 - ETA: 0s - loss: 0.1536 - acc: 0.962 - ETA: 0s - loss: 0.1508 - acc: 0.963 - ETA: 0s - loss: 0.1605 - acc: 0.958 - ETA: 0s - loss: 0.1585 - acc: 0.958 - 0s 27us/step - loss: 0.1577 - acc: 0.9592 - val_loss: 0.7443 - val_acc: 0.7674\n",
      "Epoch 30/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1334 - acc: 0.972 - ETA: 0s - loss: 0.1368 - acc: 0.967 - ETA: 0s - loss: 0.1431 - acc: 0.967 - ETA: 0s - loss: 0.1458 - acc: 0.966 - ETA: 0s - loss: 0.1481 - acc: 0.964 - 0s 27us/step - loss: 0.1479 - acc: 0.9643 - val_loss: 0.7408 - val_acc: 0.7732\n",
      "Epoch 31/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1343 - acc: 0.968 - ETA: 0s - loss: 0.1410 - acc: 0.966 - ETA: 0s - loss: 0.1452 - acc: 0.963 - ETA: 0s - loss: 0.1433 - acc: 0.964 - ETA: 0s - loss: 0.1457 - acc: 0.963 - 0s 27us/step - loss: 0.1457 - acc: 0.9638 - val_loss: 0.7484 - val_acc: 0.7742\n",
      "Epoch 32/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1235 - acc: 0.968 - ETA: 0s - loss: 0.1307 - acc: 0.970 - ETA: 0s - loss: 0.1312 - acc: 0.968 - ETA: 0s - loss: 0.1371 - acc: 0.966 - ETA: 0s - loss: 0.1395 - acc: 0.965 - 0s 27us/step - loss: 0.1399 - acc: 0.9654 - val_loss: 0.7528 - val_acc: 0.7703\n",
      "Epoch 33/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1142 - acc: 0.972 - ETA: 0s - loss: 0.1325 - acc: 0.966 - ETA: 0s - loss: 0.1318 - acc: 0.967 - ETA: 0s - loss: 0.1339 - acc: 0.967 - ETA: 0s - loss: 0.1370 - acc: 0.965 - 0s 27us/step - loss: 0.1366 - acc: 0.9662 - val_loss: 0.7844 - val_acc: 0.7693\n",
      "Epoch 34/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1388 - acc: 0.964 - ETA: 0s - loss: 0.1430 - acc: 0.961 - ETA: 0s - loss: 0.1361 - acc: 0.964 - ETA: 0s - loss: 0.1354 - acc: 0.966 - ETA: 0s - loss: 0.1355 - acc: 0.966 - 0s 27us/step - loss: 0.1361 - acc: 0.9664 - val_loss: 0.7779 - val_acc: 0.7703\n",
      "Epoch 35/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1103 - acc: 0.972 - ETA: 0s - loss: 0.1255 - acc: 0.968 - ETA: 0s - loss: 0.1273 - acc: 0.968 - ETA: 0s - loss: 0.1264 - acc: 0.968 - ETA: 0s - loss: 0.1253 - acc: 0.968 - 0s 27us/step - loss: 0.1258 - acc: 0.9679 - val_loss: 0.7785 - val_acc: 0.7654\n",
      "Epoch 36/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1223 - acc: 0.976 - ETA: 0s - loss: 0.1282 - acc: 0.966 - ETA: 0s - loss: 0.1242 - acc: 0.970 - ETA: 0s - loss: 0.1240 - acc: 0.970 - ETA: 0s - loss: 0.1261 - acc: 0.968 - 0s 26us/step - loss: 0.1274 - acc: 0.9684 - val_loss: 0.7737 - val_acc: 0.7771\n",
      "Epoch 37/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1305 - acc: 0.964 - ETA: 0s - loss: 0.1232 - acc: 0.972 - ETA: 0s - loss: 0.1201 - acc: 0.971 - ETA: 0s - loss: 0.1203 - acc: 0.970 - ETA: 0s - loss: 0.1227 - acc: 0.968 - 0s 27us/step - loss: 0.1224 - acc: 0.9688 - val_loss: 0.7879 - val_acc: 0.7644\n",
      "Epoch 38/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1363 - acc: 0.960 - ETA: 0s - loss: 0.1117 - acc: 0.975 - ETA: 0s - loss: 0.1151 - acc: 0.972 - ETA: 0s - loss: 0.1147 - acc: 0.972 - ETA: 0s - loss: 0.1173 - acc: 0.971 - 0s 27us/step - loss: 0.1169 - acc: 0.9721 - val_loss: 0.7650 - val_acc: 0.7810\n",
      "Epoch 39/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1217 - acc: 0.984 - ETA: 0s - loss: 0.1089 - acc: 0.973 - ETA: 0s - loss: 0.1062 - acc: 0.974 - ETA: 0s - loss: 0.1103 - acc: 0.973 - ETA: 0s - loss: 0.1101 - acc: 0.973 - 0s 29us/step - loss: 0.1101 - acc: 0.9740 - val_loss: 0.7560 - val_acc: 0.7771\n",
      "Epoch 40/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1100 - acc: 0.972 - ETA: 0s - loss: 0.1040 - acc: 0.978 - ETA: 0s - loss: 0.1010 - acc: 0.977 - ETA: 0s - loss: 0.1062 - acc: 0.974 - ETA: 0s - loss: 0.1076 - acc: 0.974 - 0s 27us/step - loss: 0.1084 - acc: 0.9737 - val_loss: 0.7642 - val_acc: 0.7713\n",
      "Epoch 41/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1220 - acc: 0.972 - ETA: 0s - loss: 0.0996 - acc: 0.979 - ETA: 0s - loss: 0.1049 - acc: 0.975 - ETA: 0s - loss: 0.1072 - acc: 0.975 - ETA: 0s - loss: 0.1084 - acc: 0.974 - 0s 27us/step - loss: 0.1088 - acc: 0.9745 - val_loss: 0.7847 - val_acc: 0.7615\n",
      "Epoch 42/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0987 - acc: 0.980 - ETA: 0s - loss: 0.1000 - acc: 0.979 - ETA: 0s - loss: 0.1033 - acc: 0.977 - ETA: 0s - loss: 0.1055 - acc: 0.975 - ETA: 0s - loss: 0.1063 - acc: 0.973 - 0s 27us/step - loss: 0.1064 - acc: 0.9741 - val_loss: 0.7748 - val_acc: 0.7703\n",
      "Epoch 43/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0876 - acc: 0.984 - ETA: 0s - loss: 0.0968 - acc: 0.977 - ETA: 0s - loss: 0.0992 - acc: 0.976 - ETA: 0s - loss: 0.1033 - acc: 0.975 - ETA: 0s - loss: 0.1044 - acc: 0.974 - 0s 28us/step - loss: 0.1053 - acc: 0.9739 - val_loss: 0.7706 - val_acc: 0.7771\n",
      "Epoch 44/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0890 - acc: 0.984 - ETA: 0s - loss: 0.1030 - acc: 0.975 - ETA: 0s - loss: 0.0995 - acc: 0.975 - ETA: 0s - loss: 0.0988 - acc: 0.976 - ETA: 0s - loss: 0.1016 - acc: 0.975 - 0s 27us/step - loss: 0.1016 - acc: 0.9751 - val_loss: 0.7763 - val_acc: 0.7674\n",
      "Epoch 45/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1008 - acc: 0.968 - ETA: 0s - loss: 0.0984 - acc: 0.975 - ETA: 0s - loss: 0.0969 - acc: 0.976 - ETA: 0s - loss: 0.0976 - acc: 0.976 - ETA: 0s - loss: 0.0978 - acc: 0.976 - 0s 28us/step - loss: 0.0987 - acc: 0.9760 - val_loss: 0.7746 - val_acc: 0.7722\n",
      "Epoch 46/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0855 - acc: 0.984 - ETA: 0s - loss: 0.0987 - acc: 0.977 - ETA: 0s - loss: 0.0993 - acc: 0.975 - ETA: 0s - loss: 0.0982 - acc: 0.976 - ETA: 0s - loss: 0.0988 - acc: 0.976 - 0s 27us/step - loss: 0.1011 - acc: 0.9754 - val_loss: 0.7952 - val_acc: 0.7693\n",
      "Epoch 47/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1034 - acc: 0.964 - ETA: 0s - loss: 0.0943 - acc: 0.974 - ETA: 0s - loss: 0.0952 - acc: 0.975 - ETA: 0s - loss: 0.0956 - acc: 0.975 - ETA: 0s - loss: 0.0939 - acc: 0.976 - 0s 28us/step - loss: 0.0955 - acc: 0.9749 - val_loss: 0.8013 - val_acc: 0.7722\n",
      "Epoch 48/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0997 - acc: 0.960 - ETA: 0s - loss: 0.0936 - acc: 0.973 - ETA: 0s - loss: 0.0940 - acc: 0.976 - ETA: 0s - loss: 0.0911 - acc: 0.977 - ETA: 0s - loss: 0.0925 - acc: 0.977 - 0s 27us/step - loss: 0.0919 - acc: 0.9780 - val_loss: 0.7904 - val_acc: 0.7703\n",
      "Epoch 49/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0914 - acc: 0.976 - ETA: 0s - loss: 0.0896 - acc: 0.978 - ETA: 0s - loss: 0.0907 - acc: 0.977 - ETA: 0s - loss: 0.0930 - acc: 0.977 - ETA: 0s - loss: 0.0941 - acc: 0.977 - 0s 28us/step - loss: 0.0959 - acc: 0.9762 - val_loss: 0.7979 - val_acc: 0.7722\n",
      "Epoch 50/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0744 - acc: 0.984 - ETA: 0s - loss: 0.1009 - acc: 0.970 - ETA: 0s - loss: 0.1045 - acc: 0.971 - ETA: 0s - loss: 0.0984 - acc: 0.974 - ETA: 0s - loss: 0.1001 - acc: 0.973 - 0s 27us/step - loss: 0.0999 - acc: 0.9730 - val_loss: 0.8017 - val_acc: 0.7722\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0558 - acc: 1.000 - ETA: 0s - loss: 0.0865 - acc: 0.979 - ETA: 0s - loss: 0.0853 - acc: 0.980 - ETA: 0s - loss: 0.0880 - acc: 0.979 - ETA: 0s - loss: 0.0896 - acc: 0.978 - 0s 27us/step - loss: 0.0897 - acc: 0.9786 - val_loss: 0.8196 - val_acc: 0.7634\n",
      "Epoch 52/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0818 - acc: 0.980 - ETA: 0s - loss: 0.0846 - acc: 0.979 - ETA: 0s - loss: 0.0906 - acc: 0.976 - ETA: 0s - loss: 0.0914 - acc: 0.977 - 0s 26us/step - loss: 0.0918 - acc: 0.9766 - val_loss: 0.8009 - val_acc: 0.7722\n",
      "Epoch 53/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0715 - acc: 0.988 - ETA: 0s - loss: 0.0813 - acc: 0.980 - ETA: 0s - loss: 0.0823 - acc: 0.978 - ETA: 0s - loss: 0.0842 - acc: 0.978 - ETA: 0s - loss: 0.0869 - acc: 0.978 - 0s 28us/step - loss: 0.0870 - acc: 0.9785 - val_loss: 0.8115 - val_acc: 0.7664\n",
      "Epoch 54/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0860 - acc: 0.984 - ETA: 0s - loss: 0.0751 - acc: 0.985 - ETA: 0s - loss: 0.0763 - acc: 0.984 - ETA: 0s - loss: 0.0811 - acc: 0.982 - ETA: 0s - loss: 0.0824 - acc: 0.981 - 0s 27us/step - loss: 0.0831 - acc: 0.9814 - val_loss: 0.8077 - val_acc: 0.7644\n",
      "Epoch 55/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0894 - acc: 0.976 - ETA: 0s - loss: 0.0906 - acc: 0.973 - ETA: 0s - loss: 0.0911 - acc: 0.974 - ETA: 0s - loss: 0.0918 - acc: 0.975 - ETA: 0s - loss: 0.0909 - acc: 0.975 - 0s 29us/step - loss: 0.0890 - acc: 0.9761 - val_loss: 0.8008 - val_acc: 0.7713\n",
      "Epoch 56/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0707 - acc: 0.984 - ETA: 0s - loss: 0.0770 - acc: 0.980 - ETA: 0s - loss: 0.0808 - acc: 0.978 - ETA: 0s - loss: 0.0815 - acc: 0.978 - 0s 26us/step - loss: 0.0839 - acc: 0.9768 - val_loss: 0.8188 - val_acc: 0.7732\n",
      "Epoch 57/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.1036 - acc: 0.972 - ETA: 0s - loss: 0.0819 - acc: 0.977 - ETA: 0s - loss: 0.0789 - acc: 0.980 - ETA: 0s - loss: 0.0791 - acc: 0.980 - ETA: 0s - loss: 0.0799 - acc: 0.981 - 0s 28us/step - loss: 0.0812 - acc: 0.9808 - val_loss: 0.8089 - val_acc: 0.7722\n",
      "Epoch 58/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0682 - acc: 0.988 - ETA: 0s - loss: 0.0764 - acc: 0.984 - ETA: 0s - loss: 0.0759 - acc: 0.983 - ETA: 0s - loss: 0.0765 - acc: 0.981 - ETA: 0s - loss: 0.0789 - acc: 0.980 - 0s 29us/step - loss: 0.0801 - acc: 0.9795 - val_loss: 0.8184 - val_acc: 0.7664\n",
      "Epoch 59/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0514 - acc: 0.992 - ETA: 0s - loss: 0.0699 - acc: 0.985 - ETA: 0s - loss: 0.0736 - acc: 0.983 - ETA: 0s - loss: 0.0762 - acc: 0.982 - ETA: 0s - loss: 0.0777 - acc: 0.981 - 0s 27us/step - loss: 0.0774 - acc: 0.9818 - val_loss: 0.8176 - val_acc: 0.7722\n",
      "Epoch 60/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0583 - acc: 0.988 - ETA: 0s - loss: 0.0686 - acc: 0.983 - ETA: 0s - loss: 0.0732 - acc: 0.979 - ETA: 0s - loss: 0.0741 - acc: 0.979 - ETA: 0s - loss: 0.0760 - acc: 0.979 - 0s 29us/step - loss: 0.0772 - acc: 0.9797 - val_loss: 0.8094 - val_acc: 0.7742\n",
      "Epoch 61/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0610 - acc: 0.988 - ETA: 0s - loss: 0.0783 - acc: 0.980 - ETA: 0s - loss: 0.0811 - acc: 0.978 - ETA: 0s - loss: 0.0792 - acc: 0.978 - ETA: 0s - loss: 0.0766 - acc: 0.980 - 0s 28us/step - loss: 0.0770 - acc: 0.9804 - val_loss: 0.8107 - val_acc: 0.7761\n",
      "Epoch 62/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0735 - acc: 0.988 - ETA: 0s - loss: 0.0791 - acc: 0.977 - ETA: 0s - loss: 0.0744 - acc: 0.979 - ETA: 0s - loss: 0.0762 - acc: 0.979 - ETA: 0s - loss: 0.0786 - acc: 0.979 - 0s 28us/step - loss: 0.0793 - acc: 0.9790 - val_loss: 0.8363 - val_acc: 0.7625\n",
      "Epoch 63/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0575 - acc: 0.996 - ETA: 0s - loss: 0.0750 - acc: 0.982 - ETA: 0s - loss: 0.0751 - acc: 0.982 - ETA: 0s - loss: 0.0764 - acc: 0.981 - ETA: 0s - loss: 0.0786 - acc: 0.980 - 0s 28us/step - loss: 0.0776 - acc: 0.9813 - val_loss: 0.8160 - val_acc: 0.7781\n",
      "Epoch 64/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0572 - acc: 0.988 - ETA: 0s - loss: 0.0719 - acc: 0.984 - ETA: 0s - loss: 0.0708 - acc: 0.984 - ETA: 0s - loss: 0.0708 - acc: 0.983 - ETA: 0s - loss: 0.0692 - acc: 0.983 - 0s 27us/step - loss: 0.0684 - acc: 0.9839 - val_loss: 0.8264 - val_acc: 0.7752\n",
      "Epoch 65/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0849 - acc: 0.980 - ETA: 0s - loss: 0.0717 - acc: 0.980 - ETA: 0s - loss: 0.0703 - acc: 0.981 - ETA: 0s - loss: 0.0714 - acc: 0.981 - ETA: 0s - loss: 0.0728 - acc: 0.980 - 0s 28us/step - loss: 0.0731 - acc: 0.9808 - val_loss: 0.8257 - val_acc: 0.7732\n",
      "Epoch 66/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0742 - acc: 0.972 - ETA: 0s - loss: 0.0625 - acc: 0.983 - ETA: 0s - loss: 0.0668 - acc: 0.982 - ETA: 0s - loss: 0.0676 - acc: 0.982 - ETA: 0s - loss: 0.0694 - acc: 0.980 - 0s 27us/step - loss: 0.0691 - acc: 0.9805 - val_loss: 0.8551 - val_acc: 0.7605\n",
      "Epoch 67/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0862 - acc: 0.976 - ETA: 0s - loss: 0.0709 - acc: 0.984 - ETA: 0s - loss: 0.0705 - acc: 0.983 - ETA: 0s - loss: 0.0694 - acc: 0.983 - ETA: 0s - loss: 0.0685 - acc: 0.984 - 0s 28us/step - loss: 0.0693 - acc: 0.9838 - val_loss: 0.8281 - val_acc: 0.7732\n",
      "Epoch 68/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0741 - acc: 0.984 - ETA: 0s - loss: 0.0649 - acc: 0.982 - ETA: 0s - loss: 0.0665 - acc: 0.981 - ETA: 0s - loss: 0.0672 - acc: 0.982 - ETA: 0s - loss: 0.0688 - acc: 0.982 - 0s 27us/step - loss: 0.0693 - acc: 0.9824 - val_loss: 0.8413 - val_acc: 0.7654\n",
      "Epoch 69/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0948 - acc: 0.968 - ETA: 0s - loss: 0.0792 - acc: 0.977 - ETA: 0s - loss: 0.0749 - acc: 0.980 - ETA: 0s - loss: 0.0745 - acc: 0.979 - ETA: 0s - loss: 0.0714 - acc: 0.981 - 0s 28us/step - loss: 0.0723 - acc: 0.9809 - val_loss: 0.8478 - val_acc: 0.7703\n",
      "Epoch 70/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0587 - acc: 0.984 - ETA: 0s - loss: 0.0595 - acc: 0.987 - ETA: 0s - loss: 0.0687 - acc: 0.982 - ETA: 0s - loss: 0.0690 - acc: 0.983 - ETA: 0s - loss: 0.0700 - acc: 0.982 - 0s 27us/step - loss: 0.0707 - acc: 0.9820 - val_loss: 0.8290 - val_acc: 0.7761\n",
      "Epoch 71/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0931 - acc: 0.968 - ETA: 0s - loss: 0.0645 - acc: 0.985 - ETA: 0s - loss: 0.0654 - acc: 0.985 - ETA: 0s - loss: 0.0646 - acc: 0.984 - ETA: 0s - loss: 0.0665 - acc: 0.983 - 0s 28us/step - loss: 0.0664 - acc: 0.9833 - val_loss: 0.8466 - val_acc: 0.7761\n",
      "Epoch 72/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0484 - acc: 0.988 - ETA: 0s - loss: 0.0626 - acc: 0.984 - ETA: 0s - loss: 0.0638 - acc: 0.984 - ETA: 0s - loss: 0.0635 - acc: 0.984 - ETA: 0s - loss: 0.0662 - acc: 0.983 - 0s 27us/step - loss: 0.0661 - acc: 0.9830 - val_loss: 0.8569 - val_acc: 0.7664\n",
      "Epoch 73/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0565 - acc: 0.984 - ETA: 0s - loss: 0.0612 - acc: 0.983 - ETA: 0s - loss: 0.0654 - acc: 0.981 - ETA: 0s - loss: 0.0663 - acc: 0.981 - ETA: 0s - loss: 0.0684 - acc: 0.981 - 0s 28us/step - loss: 0.0689 - acc: 0.9808 - val_loss: 0.8698 - val_acc: 0.7693\n",
      "Epoch 74/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0708 - acc: 0.988 - ETA: 0s - loss: 0.0690 - acc: 0.982 - ETA: 0s - loss: 0.0664 - acc: 0.983 - ETA: 0s - loss: 0.0687 - acc: 0.982 - ETA: 0s - loss: 0.0705 - acc: 0.980 - 0s 27us/step - loss: 0.0710 - acc: 0.9808 - val_loss: 0.8735 - val_acc: 0.7742\n",
      "Epoch 75/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0727 - acc: 0.984 - ETA: 0s - loss: 0.0678 - acc: 0.982 - ETA: 0s - loss: 0.0643 - acc: 0.983 - ETA: 0s - loss: 0.0655 - acc: 0.983 - ETA: 0s - loss: 0.0684 - acc: 0.981 - 0s 27us/step - loss: 0.0683 - acc: 0.9815 - val_loss: 0.8440 - val_acc: 0.7722\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0979 - acc: 0.972 - ETA: 0s - loss: 0.0724 - acc: 0.979 - ETA: 0s - loss: 0.0713 - acc: 0.980 - ETA: 0s - loss: 0.0701 - acc: 0.982 - ETA: 0s - loss: 0.0719 - acc: 0.981 - 0s 27us/step - loss: 0.0713 - acc: 0.9818 - val_loss: 0.8525 - val_acc: 0.7634\n",
      "Epoch 77/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0704 - acc: 0.984 - ETA: 0s - loss: 0.0632 - acc: 0.987 - ETA: 0s - loss: 0.0650 - acc: 0.985 - ETA: 0s - loss: 0.0674 - acc: 0.983 - ETA: 0s - loss: 0.0665 - acc: 0.983 - 0s 27us/step - loss: 0.0665 - acc: 0.9838 - val_loss: 0.8948 - val_acc: 0.7625\n",
      "Epoch 78/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0535 - acc: 0.988 - ETA: 0s - loss: 0.0593 - acc: 0.985 - ETA: 0s - loss: 0.0616 - acc: 0.984 - ETA: 0s - loss: 0.0626 - acc: 0.983 - ETA: 0s - loss: 0.0634 - acc: 0.983 - 0s 27us/step - loss: 0.0638 - acc: 0.9833 - val_loss: 0.8734 - val_acc: 0.7703\n",
      "Epoch 79/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0776 - acc: 0.972 - ETA: 0s - loss: 0.0595 - acc: 0.983 - ETA: 0s - loss: 0.0608 - acc: 0.983 - ETA: 0s - loss: 0.0631 - acc: 0.982 - ETA: 0s - loss: 0.0653 - acc: 0.981 - 0s 27us/step - loss: 0.0654 - acc: 0.9808 - val_loss: 0.8601 - val_acc: 0.7713\n",
      "Epoch 80/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0555 - acc: 0.988 - ETA: 0s - loss: 0.0638 - acc: 0.981 - ETA: 0s - loss: 0.0615 - acc: 0.983 - ETA: 0s - loss: 0.0604 - acc: 0.983 - ETA: 0s - loss: 0.0624 - acc: 0.982 - 0s 27us/step - loss: 0.0631 - acc: 0.9823 - val_loss: 0.8867 - val_acc: 0.7644\n",
      "Epoch 81/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0342 - acc: 1.000 - ETA: 0s - loss: 0.0583 - acc: 0.986 - ETA: 0s - loss: 0.0580 - acc: 0.985 - ETA: 0s - loss: 0.0584 - acc: 0.986 - ETA: 0s - loss: 0.0613 - acc: 0.984 - 0s 27us/step - loss: 0.0615 - acc: 0.9845 - val_loss: 0.8717 - val_acc: 0.7703\n",
      "Epoch 82/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0498 - acc: 0.996 - ETA: 0s - loss: 0.0555 - acc: 0.986 - ETA: 0s - loss: 0.0590 - acc: 0.984 - ETA: 0s - loss: 0.0594 - acc: 0.984 - ETA: 0s - loss: 0.0604 - acc: 0.983 - 0s 27us/step - loss: 0.0615 - acc: 0.9833 - val_loss: 0.8498 - val_acc: 0.7683\n",
      "Epoch 83/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0474 - acc: 0.988 - ETA: 0s - loss: 0.0586 - acc: 0.983 - ETA: 0s - loss: 0.0603 - acc: 0.982 - ETA: 0s - loss: 0.0609 - acc: 0.983 - ETA: 0s - loss: 0.0602 - acc: 0.983 - 0s 30us/step - loss: 0.0605 - acc: 0.9832 - val_loss: 0.8540 - val_acc: 0.7713\n",
      "Epoch 84/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0434 - acc: 0.988 - ETA: 0s - loss: 0.0585 - acc: 0.984 - ETA: 0s - loss: 0.0629 - acc: 0.982 - ETA: 0s - loss: 0.0613 - acc: 0.983 - ETA: 0s - loss: 0.0609 - acc: 0.983 - 0s 28us/step - loss: 0.0609 - acc: 0.9833 - val_loss: 0.8897 - val_acc: 0.7586\n",
      "Epoch 85/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0607 - acc: 0.980 - ETA: 0s - loss: 0.0580 - acc: 0.984 - ETA: 0s - loss: 0.0574 - acc: 0.984 - ETA: 0s - loss: 0.0622 - acc: 0.982 - ETA: 0s - loss: 0.0629 - acc: 0.982 - 0s 27us/step - loss: 0.0627 - acc: 0.9824 - val_loss: 0.8732 - val_acc: 0.7703\n",
      "Epoch 86/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.992 - ETA: 0s - loss: 0.0582 - acc: 0.982 - ETA: 0s - loss: 0.0667 - acc: 0.979 - ETA: 0s - loss: 0.0635 - acc: 0.981 - ETA: 0s - loss: 0.0648 - acc: 0.980 - 0s 32us/step - loss: 0.0635 - acc: 0.9816 - val_loss: 0.8695 - val_acc: 0.7713\n",
      "Epoch 87/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0566 - acc: 0.984 - ETA: 0s - loss: 0.0561 - acc: 0.985 - ETA: 0s - loss: 0.0600 - acc: 0.984 - ETA: 0s - loss: 0.0575 - acc: 0.984 - ETA: 0s - loss: 0.0633 - acc: 0.982 - 0s 28us/step - loss: 0.0621 - acc: 0.9826 - val_loss: 0.8765 - val_acc: 0.7683\n",
      "Epoch 88/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0735 - acc: 0.980 - ETA: 0s - loss: 0.0617 - acc: 0.984 - ETA: 0s - loss: 0.0591 - acc: 0.985 - ETA: 0s - loss: 0.0580 - acc: 0.985 - ETA: 0s - loss: 0.0585 - acc: 0.985 - 0s 27us/step - loss: 0.0582 - acc: 0.9858 - val_loss: 0.8771 - val_acc: 0.7674\n",
      "Epoch 89/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0593 - acc: 0.980 - ETA: 0s - loss: 0.0624 - acc: 0.983 - ETA: 0s - loss: 0.0609 - acc: 0.986 - ETA: 0s - loss: 0.0601 - acc: 0.985 - ETA: 0s - loss: 0.0603 - acc: 0.983 - 0s 28us/step - loss: 0.0607 - acc: 0.9838 - val_loss: 0.8945 - val_acc: 0.7674\n",
      "Epoch 90/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0512 - acc: 0.992 - ETA: 0s - loss: 0.0621 - acc: 0.983 - ETA: 0s - loss: 0.0612 - acc: 0.985 - ETA: 0s - loss: 0.0597 - acc: 0.985 - ETA: 0s - loss: 0.0617 - acc: 0.984 - 0s 27us/step - loss: 0.0622 - acc: 0.9839 - val_loss: 0.8894 - val_acc: 0.7722\n",
      "Epoch 91/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0409 - acc: 0.996 - ETA: 0s - loss: 0.0507 - acc: 0.988 - ETA: 0s - loss: 0.0564 - acc: 0.984 - ETA: 0s - loss: 0.0581 - acc: 0.983 - ETA: 0s - loss: 0.0592 - acc: 0.983 - 0s 27us/step - loss: 0.0590 - acc: 0.9837 - val_loss: 0.8986 - val_acc: 0.7595\n",
      "Epoch 92/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0568 - acc: 0.984 - ETA: 0s - loss: 0.0547 - acc: 0.987 - ETA: 0s - loss: 0.0530 - acc: 0.987 - ETA: 0s - loss: 0.0556 - acc: 0.985 - ETA: 0s - loss: 0.0554 - acc: 0.985 - 0s 27us/step - loss: 0.0555 - acc: 0.9855 - val_loss: 0.9218 - val_acc: 0.7595\n",
      "Epoch 93/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0735 - acc: 0.968 - ETA: 0s - loss: 0.0670 - acc: 0.978 - ETA: 0s - loss: 0.0682 - acc: 0.978 - ETA: 0s - loss: 0.0642 - acc: 0.980 - ETA: 0s - loss: 0.0637 - acc: 0.981 - 0s 28us/step - loss: 0.0624 - acc: 0.9816 - val_loss: 0.8808 - val_acc: 0.7674\n",
      "Epoch 94/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0508 - acc: 0.988 - ETA: 0s - loss: 0.0532 - acc: 0.987 - ETA: 0s - loss: 0.0547 - acc: 0.986 - ETA: 0s - loss: 0.0538 - acc: 0.986 - ETA: 0s - loss: 0.0542 - acc: 0.985 - 0s 27us/step - loss: 0.0537 - acc: 0.9857 - val_loss: 0.9264 - val_acc: 0.7674\n",
      "Epoch 95/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0523 - acc: 0.988 - ETA: 0s - loss: 0.0534 - acc: 0.987 - ETA: 0s - loss: 0.0560 - acc: 0.985 - ETA: 0s - loss: 0.0557 - acc: 0.985 - ETA: 0s - loss: 0.0569 - acc: 0.984 - 0s 27us/step - loss: 0.0586 - acc: 0.9838 - val_loss: 0.9254 - val_acc: 0.7615\n",
      "Epoch 96/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0712 - acc: 0.976 - ETA: 0s - loss: 0.0541 - acc: 0.985 - ETA: 0s - loss: 0.0553 - acc: 0.984 - ETA: 0s - loss: 0.0563 - acc: 0.985 - ETA: 0s - loss: 0.0577 - acc: 0.984 - 0s 27us/step - loss: 0.0579 - acc: 0.9843 - val_loss: 0.8871 - val_acc: 0.7722\n",
      "Epoch 97/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0527 - acc: 0.980 - ETA: 0s - loss: 0.0595 - acc: 0.984 - ETA: 0s - loss: 0.0563 - acc: 0.985 - ETA: 0s - loss: 0.0552 - acc: 0.985 - ETA: 0s - loss: 0.0555 - acc: 0.985 - 0s 28us/step - loss: 0.0561 - acc: 0.9846 - val_loss: 0.8912 - val_acc: 0.7713\n",
      "Epoch 98/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0660 - acc: 0.980 - ETA: 0s - loss: 0.0497 - acc: 0.986 - ETA: 0s - loss: 0.0519 - acc: 0.986 - ETA: 0s - loss: 0.0538 - acc: 0.985 - ETA: 0s - loss: 0.0555 - acc: 0.984 - 0s 27us/step - loss: 0.0562 - acc: 0.9847 - val_loss: 0.8941 - val_acc: 0.7752\n",
      "Epoch 99/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0751 - acc: 0.968 - ETA: 0s - loss: 0.0573 - acc: 0.981 - ETA: 0s - loss: 0.0574 - acc: 0.983 - ETA: 0s - loss: 0.0574 - acc: 0.983 - ETA: 0s - loss: 0.0585 - acc: 0.983 - 0s 29us/step - loss: 0.0575 - acc: 0.9837 - val_loss: 0.9125 - val_acc: 0.7713\n",
      "Epoch 100/100\n",
      "9199/9199 [==============================] - ETA: 0s - loss: 0.0465 - acc: 0.984 - ETA: 0s - loss: 0.0596 - acc: 0.984 - ETA: 0s - loss: 0.0613 - acc: 0.984 - ETA: 0s - loss: 0.0628 - acc: 0.983 - ETA: 0s - loss: 0.0610 - acc: 0.983 - 0s 27us/step - loss: 0.0609 - acc: 0.9830 - val_loss: 0.9042 - val_acc: 0.7742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca4d381710>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_based_model.fit(resnet50_features, y_train, batch_size=256, epochs=EPOCHS,  callbacks=callbacks, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
